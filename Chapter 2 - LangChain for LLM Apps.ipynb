{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf5a315-d7b7-4bae-9732-76ea6a0f0dea",
   "metadata": {},
   "source": [
    "# LangChain for LLM Apps\n",
    "Simply accessing LLMs via APIs has limitations. Instead combining them with other data sources and tools can enable more powerful applications. LangChain as a way to overcome LLM limitations and build innovative language-based applications.\n",
    "\n",
    "\n",
    "__AIM: Demonstrate the potential of combining recent AI advancements with a robust framework like LangChain__\n",
    "\n",
    "Challenges of using LLM on their own:\n",
    "1. Lack of external knowledge\n",
    "2. Incorrect reasoning\n",
    "3. And the inability to take action\n",
    "\n",
    "-->  LangChain provides solutions to these through integration and off-the-shelf components for specific tasks\n",
    "    - Enables dynamic, data-aware applications that go beyond what is possible by simply accessing LLMs via API calls\n",
    "\n",
    "Main Sections\n",
    "* Going beyond stochastic parrots\n",
    "* What is LangChain\n",
    "* Exploring key components of LangChain\n",
    "* How does LangChain work?\n",
    "* Comparing LangChain with other frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a58c43-3f91-4ed5-a12e-af4e2dbd8c08",
   "metadata": {},
   "source": [
    "# 2.1 Going beyond stochastic parrots\n",
    "_Problems_\n",
    "* Apparent fluency obscures serious deficiencies that constrain real-world utility\n",
    "* Concept of stochastic parrots helps to elucidate this fundamental issue\n",
    "\n",
    "Stochastic parrots refers to LLMs that can produce convincing language but lack any true comprehension of the meaning behind words (Critiques models that mindlessly mimic linguistic patterns). Without being grounded in the real world, models can produce responses that are inaccurate, irrelevant, unethical, or make little logical sense.\n",
    "\n",
    "* Scaling up compute and data does not impart reasoning capabilities or common sense.\n",
    "* LLMs struggle with challenges like compositionality gap (_Measuring and Narrowing the Compositionality Gap in Language Models_ by Ofir Press; 2023)\n",
    "* LLM cannot connect inferences or adapt responses to new situations.\n",
    "* To overcome these problems, it requires augmenting LLMs with techniques that add future comprehension\n",
    "* __Raw model scale alone cannot transform stochastic parroting into beneficial systems__\n",
    "* __Solution__:\n",
    "    * Innovations like prompting\n",
    "    * Chain-of-thought reasoning\n",
    "    * Retrieval Grounding\n",
    "    * Etc.s\n",
    " \n",
    "### Limitations\n",
    "1. __Outdated Knowledge__: LLMs rely on soley on their training data. Without external integration, they cannot provide recent real-world information\n",
    "2. __Inability to take action__: LLMs cannot perform interactive actions like searches, calculations, or lookups. This severely limits functionality.\n",
    "3. __Hallucination risks__: Insufficient knowledge on certain topics can lead to the generation of incorrect or nonsensical content by LLMs if not properly grounded.\n",
    "4. __Biases and discrimination__: Depending on the data they were trained on, LLMs can exhibit biases that can be religious, ideological, or political in nature\n",
    "5. __Lack of transparency__: The behavior of large complex models can be opaque and difficult to interpret, posing challenges to alignment with human values.\n",
    "6. __Lack of context__: LLMs may struggle to understand and incorporate context from previous prompts or conversations. They may not remember previously mentioned details or may fail to provide additional relevant information beynd the given prompt.\n",
    "\n",
    " E.g. An LLM could fluently discuss macroeconomic principles used in financial analysis, but it would fail to actually conduct analysis by retrieving current performance data and computing relevant statistics. Without dynamic lookup abilities, its financial discussion remains generic and theoretical. \n",
    "\n",
    "LLM hasn't stored the result of the calculation or hasn't encountered it often enough in the training data for it to be reliabily remembered as encoded in its weights. Therefore, it fails to correctly come up with the slution.\n",
    "\n",
    "---\n",
    "### How can we mitigate LLM limitations\n",
    "1. __Retrieval Augmentation__: This technique accesses knowledge bases to supplement an LLM's outdated training data, providing external context and reducing hallucination risk.\n",
    "2. __Chaining__: This technique integrates actions like searches and calculations.\n",
    "3. __Prompt Engineering__: This involves the careful crafting of prompts by providing critical context that guides appropriate responses.\n",
    "4. __Monitoring, filtering, and reviews__: This involves ongoing and effective oversight of emerging issues regarding the application's input and output to detect issues. Both manual reviews and automated filters then correct potential problems with the output.\n",
    "   * __Filters__: Like block lists, sensitivity classifiers, and banned word filters, can automatically flag issues.\n",
    "   * __Constitutional Principles__: monitor and filter unethical or inappropriate content.\n",
    "   * __Human reviews__: provide insight into model behavior and output.\n",
    "5. __Memory__: Retains conversation context by persisting conversation data and context across interactions.\n",
    "6. __Fine-tuning__: Training and tuning the LLM on more appropriate data for the application domain and principles. This adapts the model's behavior for its specific purpose.\n",
    "\n",
    "Raw model scale alone cannot impart compositional reasoning or other missing capabilities. Explicit techniques like elicit prompting and chain-of-thought reasoning are needed to overcome the compositionality gap. Approaches like self-ask prompting mitigate these flaws by encouraging models to methodically decompose problems.\n",
    "\n",
    "Prompting supplies the context, chaining enables inference steps, and retrieval incorporates facts.\n",
    "   \n",
    "Ongoing monintoring then catches any emerging issues, both through automation and human review. Filters act as a first line of defense. Adopting constituional AI principles also encourages building models capable of behaving ethically.\n",
    "\n",
    "Connecting LLMs. to external data further reduces hallucination risks and enhances responses with accurate, up-to-date information. \n",
    "\n",
    "With LangChain it simplify this while providing structure and oversight for responsible LLM use. They allow composing prompted model queries and data sources to surmount standalone LLM deficits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460fbe2-6a92-4148-8621-3d1d0efe89e4",
   "metadata": {},
   "source": [
    "### What is an LLM app?\n",
    "!['Figure 2.5 A traditional software application!'](fig/f2-5.png)\n",
    "In this traditional software application the client layer handles user interaction. The frontend layer handles presentation and business logic. The backend layer processes logic, APIs, computations, etc. Lastly, the database stores and retreives data.\n",
    "\n",
    "In contast LLM app is an application that utilizes an LLM to understand natural language prompts and generate responsive text outputs. LLM apps typically have the following components:\n",
    "* A client layer to collect user input as text queries or decisions.\n",
    "* Prompt engineering layer to construct prompts that guide the LLM\n",
    "* An LLM backend to analyze prompts and produce relevant text responses.\n",
    "* An output parsing layer to interpret LLM responses for the application interface\n",
    "* Optional integration with external services via function APIs, knowledge bases, and reasoning algorithms to augment the LLM's capabilities.\n",
    "\n",
    "In simplest possible cases, the frontend, parsing, and knowledge bae parts are not explicitly defined.\n",
    "\n",
    "![](fig/f2-6.png)\n",
    "\n",
    "LLM apps can integrate external services via:\n",
    "* Function APIs to access web tools and databases\n",
    "* Advanced reasoning algorithms for complex logic\n",
    "* Retrieval augmented generated via knowledge bases\n",
    "\n",
    "__Retrieval augmented generation (RAG)__ will be discuss in Chapter 5, to enhance the LLM with external knowledge. These extensions helps to expand LLM apps capabilities beyond the LLM's knowledge alone:\n",
    "* Function calling allows parameterized API requests\n",
    "* SQL functions enable conversational database queries.\n",
    "* Reasoning algorithms like chain-of-thought facilitate multi-step logic\n",
    "\n",
    "As shown in below\n",
    "\n",
    "![](fig/f2-7.png)\n",
    "\n",
    "\n",
    "- Client layer collects user text queries and decisions\n",
    "- __Prompt engineering constructs guide the LLM__, considering external knowledge or capability (or earlier interactions) without changes to the model itself.\n",
    "- The LLM backend dynamically understands and responds to the prompts based on its training.\n",
    "- Output parsing interprets the LLM text for the frontend.\n",
    "\n",
    "LLM applications are important for several reasons:\n",
    "* The LLM backend handles language in a nuanced, human-like way without hardcoded rules\n",
    "* Responses can be personalized and contextualized based on past interactions.\n",
    "* Advanced reasoning algorithms enable complex, multi-steo inference chains\n",
    "* Dynamic responses based on the LLM or on up-to-date information retrieved in real time.\n",
    "\n",
    "The key capability LLM apps use is the ability to understand nuanced language in prompts and generate coherent, human-like text responses. Hence allowing more natural interactions and workflows compare to traditional code.\n",
    "\n",
    "LLM Apps:\n",
    "- Chatbots and virtual assistants\n",
    "- Intelligent search engines\n",
    "- Automated content creation\n",
    "- Question answering\n",
    "- Sentiment analysis\n",
    "- text summarization\n",
    "- Data analysis\n",
    "- Code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813fab0d-b346-41fe-be41-6975c2e7d464",
   "metadata": {},
   "source": [
    "# 2.2 What is LangChain\n",
    "\n",
    "Created in 2022 by Harrison Chase, an open-source Python framework for building LLM-powered applications. Provides developers with modular, easy-to-use components for connecting language models with external data sources and services. LangChain simplifies the development of sophisticated LLM aplication by providing reusable components and pre-assembled chains. Its modular architecture abstracts access to LLMs and external services into a unified interface. Developers can combine these building blocks to carry out complex workflows.\n",
    "\n",
    "Building impactful LLM apps involves challenges like prompt engineering, bias mitigation, productionizing and integrating external data. Also allow conversational context and persistence through agents and memory. LangChain supports chains, agents, tools, and memory allows developers to build applications that can interact with their environment in a more sophisticated way and store and reuse information over time. Modular design makes it easy to build complex applications that can be adapted to a variety of domains.\n",
    "\n",
    "The key benefits LangChain offers developers are:\n",
    "* __Modular architecture__ for flexible and adaptable LLM integrations.\n",
    "* __Chaining together__ multiple services beyond just LLMs.\n",
    "* __Goal-driven agent__ interactions instead of isolated calls.\n",
    "* __Memory and persistence__ for statefulness across executions\n",
    "* __Open-source access__ and community support\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/platforms/\n",
    "\n",
    "In a broader ecosystem, LangSmith is a platform that complements LangChain by providing robust debugging, testing, and monitoring capabilities for LLM applications. E.g., Debug new chains by viewing detailed execution traces. LLamaHub is a library of data loaders, readers, and tools created by the LlamaIndex community. It provides utilities to easily connect LLMs to diverse knowledge sources. LlamaHub simplifies the creation of customized data agents to unlock LLM capabilities.\n",
    "\n",
    "__LangChainHub__ is a central repository for sharing artifacts like prompts, chains, and agents used in LangChain. Just like Hugging Face Hub, it aims to be a one-stop resource for discovering high-quality building blocks to compose complex LLM apps.\n",
    "\n",
    "__LangFlow__ and __Flowise_ are UIs that allow chaining LangChain components in an executable flowchart by dragging sidebar components onto the canvas and connecting them together to create your pipeline.\n",
    "\n",
    "![](fig/f2-9.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab7fcb7-28e1-43b4-8322-68dbd59c88ea",
   "metadata": {},
   "source": [
    "### 2.3 Exploring key components of LangChain\n",
    "\n",
    "What are chains?\n",
    "---\n",
    "\n",
    "- Chains are critical concepts in LangChain for composing modular component into reusable pipelines. E.g. putting together multiple LLM calls and other components in a sequence to create complex applications for things like chatbot-like social interactions, data extraction, and data analysis.\n",
    "- A sequence of calls to components, which can inlude other chains\n",
    "- E.g. _promptTemplate_ which passes a formatted response to a language model.\n",
    "\n",
    "  __Prompt chaining__ is a technique that can be used to improve the performance of LangChain applications, which involves chaining together multiple prompts to autocomplete a more complex response. E.g. complex chains integrate models with tools like LLMMath, for math-related queries, or SQLDatabaseChain, for querying databases. These are called utility chains, because they combine language models with specific tools.\n",
    "\n",
    "Chains can also enforce policies, like moderating toxic outputs or aligning with ethical priniciples. LangChain implements chains to make sure the content of the output is not toxic.\n",
    "\n",
    "_LLMCheckerChain_ verifies statements to reduce inaccurate responses using a technique called self-reflection. The LLMCheckerChain can prevent hallucinations and reduce inaccurate responses by verifying the assumptions underlying the provided statements and questions.\n",
    "\n",
    "A few chains can make autonomous decisions, e.g. agents, router chains can decide which tool to use based on their description. _RouterChain_ can  dynamically select which retrieval system, such as prompts or indexes, to use.\n",
    "\n",
    "Benefits of Chains:\n",
    "1. __Modularity__: Logic is divided into reusable components\n",
    "2. __Composability__: Components can be sequenced flexibly\n",
    "3. __Readability__: Each step in a pipeline is clear\n",
    "4. __Maintainability__: Steps can be added, removed, and swapped\n",
    "5. __Reusability__: Common pipelines become configurable chains\n",
    "6. __Tool integration__: Easily incorporate LLMs, databases, APIs, etc\n",
    "7. __Productivity__: Quickly build prototypes of configurable chains\n",
    "\n",
    "Typically, devloping a LangChain chain involves breaking down workflow into logical steps, like data loading, processing, model querying and so on.\n",
    "\n",
    "What are agents?\n",
    "---\n",
    "Agents are key conecpt in LangChain for creating systems that interact dynamically with users and environments over time. \n",
    "- Agent is an autonomous software entity that is capable of taking actions to accomplish goals and tasks.\n",
    "\n",
    "Chains and agents are similar concepts and it's worth unpicking their differences. Both chains and agents allow the compositionality of LLMs and other components to work together but in different ways. Both extend LLMs, but agents do so by orchestrating chains while chains compose lower-level modules. While chain defines reusable logic by sequencing components, agents leverage chains to take goal-driven actions. Agents combine and orchestrate chains.\n",
    "\n",
    "- Observes the environment, decides which chain to execute based on that observation, takes the chain's specified action and repeats.\n",
    "- They act as reasoning engines to decide which actions to take using LLMs.\n",
    "- Tools are functions the agent calls to take real-world actions. Providing the right tools and effectively describing them is critical for agents to accomplish goals.\n",
    "- The agent executor runtime ochestrates the loop of querying the agent, executing tool actions, and feeding observations back. Thus handles the lower-level complexities like error handling, logging, and parsing.\n",
    "\n",
    "Agents provide several key benefits:\n",
    "* __Goal-oriented execution__: Agents can plan chains of logic targeting specific goals.\n",
    "* __Dynamic Responses__: Observing environment changes lets agents react and adapt.\n",
    "* __Statefulness__: Agents can maintain memory and context across interactions.\n",
    "* __Robustness__:Errors can be handled by catching execptions and trying alternate chains.\n",
    "* __Composition__: Agent logic combines reusable component chains\n",
    "\n",
    "Together, this enables agents to handle complex, mulistep workflows and continuously interactive applications like chatbots.\n",
    "\n",
    "![](fig/f2-10.png)\n",
    "\n",
    "A key limitation of agents and chains is their statelessness - each execution occurs in isolation without retaining prior context. This is where memory becomes important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba98e7-982e-4d76-bccc-548db17638e2",
   "metadata": {},
   "source": [
    "What is memory?\n",
    "---\n",
    "In LangChain, memory refers to the persisting state between executions of a chain or agent. Robust memory approaches unlock key benefits for building conversationaland interactive applications.\n",
    "\n",
    "Rather than treating each user input as an isolated prompt, chains can pass conversational memory to models on each call to provide consistency. Agents can also persist facts, relationships, and deductions about the world in memory. This knowldge remains available even as real-world conditions change, keeping the agent contextually informed. Memory of objectives and completed tasks allows agents to track progress on multi-step goals across conversations. \n",
    "\n",
    "By retaining information in memory, it reduces the number of calls to LLMs for reptitive information. Thus lowers API usage and cost, while still providing the agent or chain with the needed context.\n",
    "\n",
    "LangChain provides a standard interface for memory, integrations with storage options like databases, and design patterns for effectively incorporating memory into chains and agents.\n",
    "\n",
    "Several memory options exist - for example:\n",
    "* _ConversationBufferMemory_ stores all messages in model history. This increases latency and costs.\n",
    "* _ConversationBufferWindowMemory_ retains only recent messages.\n",
    "* _ConversationKGMemory_ summarizes exchanges as a knowledge graph for integration into prompts.\n",
    "* _EntitiyMemory_ backed by a database persists agent state and facts.\n",
    "\n",
    "LangChain integrates many database options for durable storage:\n",
    "* SQL options like Postgres and SQLite enable relational data modeling\n",
    "* NoSQL choices like MongoDB and Cassandra facilitate scalable unstructured data.\n",
    "* Redis provides an in-memory database for high-performance caching.\n",
    "* Managed cloud services like AWS DynamoDB remove infrastructure burdens.\n",
    "\n",
    "LangChain's memory integrations, from short-term caching to long-term databases, enable the building of stateful, context-aware agents. LangChain comes with a long list of tools that we can use in applications.\n",
    "\n",
    "\n",
    "What are tools?\n",
    "---\n",
    "Tools provide modular interfaces for agents to integrate external services like databases and APIs. Tools can be combined with models to extend their capability. LangChain offers tools like document loaders, indexes, and vector stores, which facilitate the retrieval and storage of data for augmenting data retrieval in LLMs. \n",
    "\n",
    "E.g. of tools:\n",
    "* __Machine translator__: A language model can use a machine translator to better comprehend and process text in multiple languages.Allowing non-translation-dedicated language models to understand and answer questions in different languages.\n",
    "* __Calculator__: Language models can utilize a simple calculator tool to solve math problems. The calculator allow arithmetic operations, allowing the model to accurately solve mathematical queries in datasets specifically designed for math problem-solving.\n",
    "* __Maps__: By connecting with the Bing Map API or similar services, language models can retrieve location information, assist with route planning, provide driving distance calculations, and offer details about nearby points of interest.\n",
    "* __Weather__: Weather APIs provide language models with real-time weather information for cities worldwide. Models can answer queries about current weather conditions or forecast the weather for specific locations within varying time periods.\n",
    "* __Stocks__: Connecting with stock market APIs like Alpha Vantage allows language models to query specific stock market information such as opening and closing prices, highest and lowest prices, and more.\n",
    "* __Slides__: Language models equipped with slide-making tools can create slides using high-level semantics provided by APIs such as the python pptx library or image retrieval from the internet based on given topics.These tools facilitate tasks related to slide creaton that are required in various professional fields.\n",
    "* __Table processing__: APIs built with pandas DataFrames enable language models to perform data analysis and visualization tasks on tables. By connecting to these tools, models can provide users with a more streamlined an natural experience for handling tabular data.\n",
    "* __Knowledge graphs__: Language models can query knowledge graphs using APIs that mimic human querying processes, such as finding candidate entities or relations, sending SPARQL queries, and retrieving results. These tools assist in answering questions based on factual knowledge stored in knowledge graphs.\n",
    "* __Search Engine__: By utilizing search engine APIs like Bing Search, language models can interact with search engines to extact information and provide answers to real-time queries. These tools enhance the model's ability to gather information from the web and deliver accurate responses.\n",
    "* __Wikipedia__: Language models equipped with Wikipedia search tools can search specific entities on Wiki pages, lookup keywords within a page, or disambiguate entities with similar names.\n",
    "* __Online Shopping__: Connecting language models with onine shopping tools allows them to perform actions like searching for items, loading detailed information about products, selecting item features, going through shopping pages, and making purchase decisions based on specific user instructions.\n",
    "\n",
    "Additional tools include AI painting, 3D Model Construction, Chemical Properties and database tools that facilitate natural language access to database data for executing SQL queries and retrieving results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9552f9-f967-4e7b-a063-92e028885ee8",
   "metadata": {},
   "source": [
    "# 2.4 How does LangChain work?\n",
    "\n",
    "LangChain framework simplifies building sophisticated LLM applications by providing modular components that facilitate connecting alnguage models with other data and services. The framwork organizes capabilities into modules spanning from basic LLM interaction to complex reasoning and persistence.\n",
    "\n",
    "Components can be combined into pipelines also called chains that sequence the following actions:\n",
    "- Loading documents\n",
    "- Embedding for retrieval\n",
    "- Querying LLMs\n",
    "- Parsing outputs\n",
    "- Writing memory\n",
    "\n",
    "Chains match modules to application goals, while agents leverage chains for goal-directed interactions with users. They repeatedly execute actions based on observations, plan optimal logic chains, and persist memory across conversations.\n",
    "\n",
    "Modules ranging from simple to advanced:\n",
    "* __LLMs and chat models__: Provide interfaces to connect and query language models like GPT-3. Support async, streaming, and batch requests.\n",
    "* __Documents loaders__: Ingest data from sources into documents with text and metadata. Enable loading files, webpages, videos, etc.\n",
    "* __Document transformers__: Manipulate documents via splitting, combining, filtering, translating, etc. Help adapt data for models.\n",
    "* __Text embeddings__: Create vector representations of text for semantic search.Different methods for embedding documents vs. queries.\n",
    "* __Vector stores__: Store embedded document vectors for efficient similarity search and retrieval.\n",
    "* __Retrievers__: General interface to return documents based on a query. Can leverage vector stores.\n",
    "* __Tools__: Interfaces that agents use to interact with external systems.\n",
    "* __Agents__: Goal-driven systems that use LLMs to plan actions based on environment observations.\n",
    "* __Toolkits__: Initialize groups of tools that share resources like databases.\n",
    "* __Memory__: Persist information across conversations and workflows by reading/writing session data.\n",
    "* __Callbacks__: Hook into pipeline stages for logging, monintoring, streaming, and others. Callbacks allow monitoring chains.\n",
    "\n",
    "LangChain offers interfaces to connect with and query LLM like GPT-3 and chat models. These interfaces support asynchronous requests, streaming responses, and batch queries. This provides a flexible API for integrating different language models.\n",
    "\n",
    "Although LangChain doesn't supply models itself, it support integration through LLM wrappers with various language model providers, enabling the app to interact with chat models as well as text embedding model providers. Supported providers include OpenAI, HuggingFace, Azure, and Anthropic. Providing a standardized interface means being able to effortlessly swap out models to save money and energy or get better performance.\n",
    "\n",
    "A core building block of LangChain is the prompt class, which allows users to interact with LLMs by providing concise instructions or examples. Prompt engineering helps optimize prompts for optimal model performance. Templates give flexibility in terms of input and the available collection of prompts is battle-tested in range of applications.\n",
    "\n",
    "_For code examples demonstrating real-world use cases: https://python.langchain.com/docs/use_cases/_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5c5e3-6b00-4181-8481-a92ad3894ce4",
   "metadata": {},
   "source": [
    "# 2.5 Comparing LangChain with other frameworks\n",
    "\n",
    "There are several open-source frameworks for building dynamic LLM applications. They all offer value in developing cutting -edge LLM applications.\n",
    "E.g.\n",
    "- Haystack\n",
    "- LangChain\n",
    "- LLamaIndex (GPTIndex)\n",
    "- SuperAGI\n",
    "- Autogen (Microsoft)\n",
    "\n",
    "LlamaIndex focuses on advanced retrieval rather than on broader aspects of LLM apps. Similarly Haystack focuses on creating large-scale search systems with components designed specifically for scalable information retrieval using retrievers, readers and other data handlers combined with semantic indexing via pre-trained models.\n",
    "\n",
    "LangChain works by chaining LLMs together using agents to delegate actions to models. Its use cases emphasize prompt optimization and context-aware information retrieval/generation.\n",
    "\n",
    "SuperAGI is similar to LangChain, even has a marketplace. However, is not as extensive and well supported as LangChain.\n",
    "\n",
    "AutoGen simplifies the building, ochestrating, and optimizing of cmplex workflows powered by LLMs. Its key innovation is enabling customizable conversational agents that automate coordination between different LLMs, humans, and tools via automated chat. AutoGen streamlines agent definition and interaction to automatically compose optimal LLM-based workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49738c8-d564-4089-a864-3c8ae396aaee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
